[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "",
    "text": "Yixin Luo"
  },
  {
    "objectID": "index.html#eda",
    "href": "index.html#eda",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nimport torch\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\n\n## Load data\ndf = pd.read_csv('~/Downloads/tripadvisor_hotel_reviews.csv')\n\n\n## distribution of ratings\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x='Rating', palette='viridis')\nplt.title('Distribution of Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\nDistribution of Ratings: The bar chart displays the frequency of ratings from 1 to 5, highlighting that the majority of users gave a rating of 5.\n\n\nCode\n## Check the length of reviews\ndf[\"word_length\"] = df[\"Review\"].apply(len)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"word_length\", hue=\"Rating\", multiple=\"stack\", palette=\"bright\")\nplt.title('Distribution of Text Lengths by Class', fontsize=16)\nplt.xlabel('Text Length', fontsize=14)\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nDistribution of Text Lengths by Class: The histogram shows the distribution of review text lengths, segmented by rating class, revealing that most reviews are short across all ratings.\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\nimport re\n\n# Step 1: Preprocess the text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    return text\n\ndf['cleaned_review'] = df['Review'].apply(clean_text)\n\n# Step 2: Tokenize the text and count words\nall_words = ' '.join(df['cleaned_review']).split()  # Combine all reviews and split into words\nword_counts = Counter(all_words)  # Count word frequencies\n\n# Step 3: Get the most common words\nmost_common_words = word_counts.most_common(20)  # Top 20 most common words\nprint(\"Most Common Words:\", most_common_words)\n\n# Convert to a DataFrame for better readability\nmost_common_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\nprint(most_common_df)\n\n# Horizontal visualization\nplt.figure(figsize=(10, 6))\nplt.barh(most_common_df['Word'], most_common_df['Frequency'], color='skyblue')\nplt.title('Top 20 Most Common Words in Reviews', fontsize=16)\nplt.xlabel('Frequency', fontsize=12)\nplt.ylabel('Words', fontsize=12)\nplt.gca().invert_yaxis()  # Invert y-axis to show highest frequency on top\nplt.tight_layout()\nplt.show()\n\n\nMost Common Words: [('hotel', 48864), ('room', 34324), ('not', 31525), ('great', 21094), ('nt', 19000), ('good', 16986), ('staff', 16213), ('stay', 15158), ('did', 13895), ('just', 12592), ('nice', 12409), ('rooms', 12024), ('no', 11620), ('location', 11043), ('stayed', 10469), ('service', 9975), ('time', 9824), ('night', 9728), ('beach', 9592), ('day', 9541)]\n        Word  Frequency\n0      hotel      48864\n1       room      34324\n2        not      31525\n3      great      21094\n4         nt      19000\n5       good      16986\n6      staff      16213\n7       stay      15158\n8        did      13895\n9       just      12592\n10      nice      12409\n11     rooms      12024\n12        no      11620\n13  location      11043\n14    stayed      10469\n15   service       9975\n16      time       9824\n17     night       9728\n18     beach       9592\n19       day       9541\n\n\n\n\n\n\n\nTop 20 Most Common Words in Reviews: The bar chart identifies the most frequently used words in hotel reviews, with “hotel” and “room” being the most common, indicating key topics of discussion."
  },
  {
    "objectID": "index.html#text-cleaning",
    "href": "index.html#text-cleaning",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Text Cleaning",
    "text": "Text Cleaning\n\nremove the big comments\n\n\nCode\n# remove the big comments\ndf = df[df['word_length'] &lt;= 3000]\n\ndf[\"word_length\"] = df[\"Review\"].apply(len)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"word_length\", hue=\"Rating\", multiple=\"stack\", palette=\"bright\")\nplt.title('Distribution of Text Lengths by Class', fontsize=16)\nplt.xlabel('Text Length', fontsize=14)\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n\n# Text cleaning function\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n    return text\n\n# Apply text cleaning\ndf.loc[:, 'cleaned_review'] = df['Review'].apply(clean_text)\nprint(df)\n\n\n                                                  Review  Rating  word_length  \\\n0      nice hotel expensive parking got good deal sta...       4          593   \n1      ok nothing special charge diamond member hilto...       2         1689   \n2      nice rooms not 4* experience hotel monaco seat...       3         1427   \n3      unique, great stay, wonderful time hotel monac...       5          600   \n4      great stay great stay, went seahawk game aweso...       5         1281   \n...                                                  ...     ...          ...   \n20485  not impressed unfriendly staff checked asked h...       2          616   \n20486  best kept secret 3rd time staying charm, not 5...       5          733   \n20487  great location price view hotel great quick pl...       4          306   \n20488  ok just looks nice modern outside, desk staff ...       2          443   \n20490  people talking, ca n't believe excellent ratin...       2          620   \n\n                                          cleaned_review  \n0      nice hotel expensive parking got good deal sta...  \n1      ok nothing special charge diamond member hilto...  \n2      nice rooms not experience hotel monaco seattle...  \n3      unique great stay wonderful time hotel monaco ...  \n4      great stay great stay went seahawk game awesom...  \n...                                                  ...  \n20485  not impressed unfriendly staff checked asked h...  \n20486  best kept secret rd time staying charm not sta...  \n20487  great location price view hotel great quick pl...  \n20488  ok just looks nice modern outside desk staff n...  \n20490  people talking ca nt believe excellent ratings...  \n\n[20196 rows x 4 columns]"
  },
  {
    "objectID": "index.html#sentiment-analysis",
    "href": "index.html#sentiment-analysis",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\nCode\n## Define aspects\naspects = ['location', 'service', 'room', 'staff']\n\nimport pandas as pd\nfrom transformers import pipeline\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\ndf_sample = df.sample(1000, random_state=42) \ndf_sample['cleaned_review'] = df_sample['Review'].apply(clean_text)\n\n# Initialize DistilBERT pipeline\naspect_sentiment_pipeline = pipeline(\n    \"text-classification\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    truncation=True,\n    padding=True\n)\n\n# Function to handle missing aspects and perform sentiment analysis\ndef analyze_aspect_sentiments_with_missing_handling(review, aspects):\n    aspect_sentiments = {}\n    for aspect in aspects:\n        # Check if the aspect is mentioned in the review\n        if aspect in review:\n            text = f\"{review} What do you think about the {aspect}?\"\n            result = aspect_sentiment_pipeline(text)\n            aspect_sentiments[aspect] = {\n                'label': result[0]['label'],  # Positive, Neutral, or Negative\n                'score': result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']\n            }\n        else:\n            # Assign default value if aspect is not mentioned\n            aspect_sentiments[aspect] = {\n                'label': 'Not Mentioned',\n                'score': None\n            }\n    return aspect_sentiments\n# Apply the function using multithreading for efficiency\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(tqdm(executor.map(lambda x: analyze_aspect_sentiments_with_missing_handling(x, aspects), \n                                     df_sample['cleaned_review']), \n                        total=len(df_sample)))\n\n# Add results to the DataFrame\ndf_sample['aspect_sentiments'] = results\n\n# Extract aspect scores into a separate DataFrame\naspect_scores_df = pd.json_normalize(df_sample['aspect_sentiments'])\n\nprint(df_sample[['Review', 'aspect_sentiments']].head())\n\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  0%|          | 1/1000 [00:00&lt;02:44,  6.09it/s]  0%|          | 2/1000 [00:00&lt;03:26,  4.83it/s]  0%|          | 4/1000 [00:00&lt;02:43,  6.10it/s]  1%|          | 9/1000 [00:00&lt;01:05, 15.07it/s]  1%|          | 12/1000 [00:00&lt;01:03, 15.64it/s]  1%|▏         | 14/1000 [00:01&lt;01:12, 13.58it/s]  2%|▏         | 17/1000 [00:01&lt;01:29, 10.98it/s]  2%|▏         | 21/1000 [00:01&lt;01:23, 11.77it/s]  3%|▎         | 28/1000 [00:01&lt;00:48, 20.03it/s]  3%|▎         | 31/1000 [00:02&lt;00:47, 20.28it/s]  4%|▎         | 36/1000 [00:02&lt;00:45, 21.05it/s]  4%|▍         | 39/1000 [00:03&lt;01:47,  8.92it/s]  6%|▌         | 56/1000 [00:03&lt;00:45, 20.87it/s]  6%|▌         | 60/1000 [00:03&lt;00:49, 18.97it/s]  7%|▋         | 68/1000 [00:03&lt;00:38, 24.11it/s]  7%|▋         | 72/1000 [00:04&lt;00:41, 22.17it/s]  8%|▊         | 75/1000 [00:04&lt;01:05, 14.14it/s]  9%|▉         | 91/1000 [00:05&lt;00:38, 23.58it/s] 10%|█         | 100/1000 [00:05&lt;00:37, 23.74it/s] 10%|█         | 105/1000 [00:05&lt;00:37, 24.05it/s] 11%|█         | 109/1000 [00:06&lt;00:59, 14.97it/s] 12%|█▏        | 120/1000 [00:06&lt;00:40, 21.74it/s] 12%|█▎        | 125/1000 [00:06&lt;00:37, 23.04it/s] 13%|█▎        | 129/1000 [00:07&lt;00:41, 20.84it/s] 13%|█▎        | 133/1000 [00:07&lt;00:43, 19.87it/s] 14%|█▍        | 138/1000 [00:07&lt;00:39, 21.93it/s] 15%|█▍        | 147/1000 [00:07&lt;00:37, 23.02it/s] 15%|█▌        | 150/1000 [00:07&lt;00:37, 22.47it/s] 15%|█▌        | 153/1000 [00:08&lt;00:41, 20.21it/s] 16%|█▌        | 156/1000 [00:08&lt;00:44, 19.17it/s] 16%|█▌        | 160/1000 [00:08&lt;00:43, 19.43it/s] 17%|█▋        | 167/1000 [00:08&lt;00:33, 25.07it/s] 17%|█▋        | 170/1000 [00:09&lt;01:13, 11.34it/s] 19%|█▊        | 186/1000 [00:09&lt;00:39, 20.62it/s] 19%|█▉        | 191/1000 [00:10&lt;00:39, 20.45it/s] 19%|█▉        | 194/1000 [00:10&lt;00:42, 18.78it/s] 20%|██        | 200/1000 [00:10&lt;00:47, 16.96it/s] 20%|██        | 202/1000 [00:11&lt;01:11, 11.23it/s] 21%|██▏       | 214/1000 [00:11&lt;00:38, 20.50it/s] 22%|██▏       | 218/1000 [00:11&lt;00:41, 18.89it/s] 22%|██▏       | 221/1000 [00:12&lt;00:42, 18.20it/s] 23%|██▎       | 228/1000 [00:12&lt;00:49, 15.61it/s] 24%|██▎       | 237/1000 [00:13&lt;00:43, 17.39it/s] 25%|██▍       | 247/1000 [00:13&lt;00:29, 25.21it/s] 25%|██▌       | 253/1000 [00:13&lt;00:25, 28.96it/s] 26%|██▌       | 258/1000 [00:13&lt;00:29, 25.06it/s] 26%|██▌       | 262/1000 [00:14&lt;00:43, 17.01it/s] 27%|██▋       | 269/1000 [00:14&lt;00:35, 20.54it/s] 27%|██▋       | 272/1000 [00:14&lt;00:43, 16.65it/s] 28%|██▊       | 275/1000 [00:14&lt;00:44, 16.25it/s] 28%|██▊       | 278/1000 [00:14&lt;00:40, 17.75it/s] 28%|██▊       | 281/1000 [00:15&lt;00:37, 19.33it/s] 28%|██▊       | 284/1000 [00:15&lt;00:46, 15.30it/s] 29%|██▉       | 292/1000 [00:15&lt;00:34, 20.42it/s] 30%|██▉       | 295/1000 [00:15&lt;00:39, 17.80it/s] 30%|██▉       | 297/1000 [00:16&lt;00:58, 11.96it/s] 31%|███       | 306/1000 [00:16&lt;00:34, 20.15it/s] 31%|███       | 311/1000 [00:16&lt;00:32, 21.31it/s] 31%|███▏      | 314/1000 [00:17&lt;00:46, 14.84it/s] 32%|███▏      | 320/1000 [00:17&lt;00:37, 17.91it/s] 32%|███▏      | 324/1000 [00:17&lt;00:51, 13.07it/s] 34%|███▎      | 337/1000 [00:18&lt;00:28, 23.67it/s] 34%|███▍      | 341/1000 [00:18&lt;00:29, 22.71it/s] 34%|███▍      | 345/1000 [00:18&lt;00:33, 19.77it/s] 35%|███▌      | 352/1000 [00:18&lt;00:32, 19.76it/s] 36%|███▌      | 358/1000 [00:19&lt;00:32, 19.96it/s] 36%|███▋      | 365/1000 [00:19&lt;00:27, 22.72it/s] 37%|███▋      | 368/1000 [00:19&lt;00:37, 16.92it/s] 38%|███▊      | 376/1000 [00:19&lt;00:25, 24.35it/s] 38%|███▊      | 380/1000 [00:20&lt;00:28, 21.95it/s] 38%|███▊      | 384/1000 [00:20&lt;00:34, 17.99it/s] 39%|███▉      | 388/1000 [00:21&lt;00:52, 11.69it/s] 40%|███▉      | 397/1000 [00:21&lt;00:34, 17.25it/s] 40%|████      | 402/1000 [00:21&lt;00:30, 19.36it/s] 40%|████      | 405/1000 [00:21&lt;00:29, 20.00it/s] 41%|████      | 409/1000 [00:21&lt;00:26, 22.37it/s] 41%|████      | 412/1000 [00:21&lt;00:25, 23.32it/s] 42%|████▏     | 415/1000 [00:22&lt;00:39, 14.97it/s] 42%|████▏     | 420/1000 [00:22&lt;00:29, 19.73it/s] 42%|████▎     | 425/1000 [00:22&lt;00:31, 18.05it/s] 43%|████▎     | 428/1000 [00:22&lt;00:29, 19.32it/s] 43%|████▎     | 432/1000 [00:23&lt;00:29, 19.36it/s] 44%|████▎     | 435/1000 [00:23&lt;00:52, 10.72it/s] 45%|████▍     | 446/1000 [00:24&lt;00:36, 15.24it/s] 45%|████▍     | 448/1000 [00:24&lt;00:37, 14.64it/s] 45%|████▌     | 453/1000 [00:24&lt;00:37, 14.55it/s] 46%|████▌     | 461/1000 [00:25&lt;00:33, 16.26it/s] 47%|████▋     | 466/1000 [00:25&lt;00:37, 14.31it/s] 47%|████▋     | 468/1000 [00:26&lt;00:43, 12.21it/s] 47%|████▋     | 474/1000 [00:26&lt;00:32, 16.39it/s] 48%|████▊     | 480/1000 [00:26&lt;00:24, 21.32it/s] 48%|████▊     | 483/1000 [00:26&lt;00:24, 21.23it/s] 49%|████▊     | 486/1000 [00:26&lt;00:25, 20.54it/s] 49%|████▉     | 489/1000 [00:27&lt;00:38, 13.39it/s] 50%|████▉     | 495/1000 [00:27&lt;00:28, 17.73it/s] 50%|████▉     | 498/1000 [00:27&lt;00:44, 11.18it/s] 51%|█████     | 509/1000 [00:27&lt;00:23, 20.91it/s] 51%|█████▏    | 513/1000 [00:28&lt;00:23, 20.99it/s] 52%|█████▏    | 517/1000 [00:28&lt;00:21, 22.56it/s] 52%|█████▏    | 521/1000 [00:28&lt;00:30, 15.58it/s] 53%|█████▎    | 528/1000 [00:28&lt;00:21, 21.95it/s] 53%|█████▎    | 532/1000 [00:29&lt;00:23, 19.56it/s] 54%|█████▎    | 535/1000 [00:29&lt;00:43, 10.64it/s] 55%|█████▌    | 550/1000 [00:30&lt;00:20, 22.24it/s] 56%|█████▌    | 555/1000 [00:30&lt;00:19, 22.74it/s] 56%|█████▌    | 559/1000 [00:30&lt;00:18, 23.84it/s] 56%|█████▋    | 563/1000 [00:30&lt;00:18, 23.07it/s] 57%|█████▋    | 567/1000 [00:31&lt;00:28, 14.96it/s] 58%|█████▊    | 577/1000 [00:31&lt;00:21, 20.00it/s] 58%|█████▊    | 585/1000 [00:31&lt;00:15, 26.90it/s] 59%|█████▉    | 590/1000 [00:32&lt;00:32, 12.54it/s] 61%|██████    | 611/1000 [00:32&lt;00:14, 26.69it/s] 62%|██████▏   | 618/1000 [00:33&lt;00:17, 22.43it/s] 62%|██████▎   | 625/1000 [00:33&lt;00:19, 18.93it/s] 63%|██████▎   | 633/1000 [00:34&lt;00:15, 23.69it/s] 64%|██████▍   | 638/1000 [00:34&lt;00:17, 21.28it/s] 64%|██████▍   | 644/1000 [00:34&lt;00:14, 24.67it/s] 65%|██████▍   | 648/1000 [00:34&lt;00:16, 21.04it/s] 65%|██████▌   | 653/1000 [00:35&lt;00:21, 15.91it/s] 66%|██████▋   | 663/1000 [00:35&lt;00:14, 23.44it/s] 67%|██████▋   | 667/1000 [00:35&lt;00:16, 20.54it/s] 67%|██████▋   | 672/1000 [00:35&lt;00:14, 22.99it/s] 68%|██████▊   | 676/1000 [00:36&lt;00:20, 15.51it/s] 68%|██████▊   | 682/1000 [00:36&lt;00:15, 20.10it/s] 69%|██████▊   | 686/1000 [00:37&lt;00:21, 14.92it/s] 69%|██████▉   | 693/1000 [00:37&lt;00:14, 20.58it/s] 70%|██████▉   | 697/1000 [00:37&lt;00:13, 22.32it/s] 70%|███████   | 701/1000 [00:37&lt;00:18, 16.13it/s] 70%|███████   | 704/1000 [00:38&lt;00:21, 13.65it/s] 71%|███████▏  | 713/1000 [00:38&lt;00:13, 22.05it/s] 72%|███████▏  | 717/1000 [00:38&lt;00:13, 20.35it/s] 72%|███████▏  | 720/1000 [00:39&lt;00:19, 14.01it/s] 73%|███████▎  | 729/1000 [00:39&lt;00:12, 22.52it/s] 73%|███████▎  | 733/1000 [00:39&lt;00:11, 24.24it/s] 74%|███████▎  | 737/1000 [00:39&lt;00:10, 24.03it/s] 74%|███████▍  | 741/1000 [00:39&lt;00:14, 18.21it/s] 74%|███████▍  | 744/1000 [00:39&lt;00:14, 17.56it/s] 75%|███████▌  | 752/1000 [00:40&lt;00:11, 21.37it/s] 76%|███████▌  | 756/1000 [00:40&lt;00:11, 20.67it/s] 76%|███████▋  | 764/1000 [00:40&lt;00:08, 28.08it/s] 77%|███████▋  | 768/1000 [00:41&lt;00:16, 14.12it/s] 78%|███████▊  | 779/1000 [00:41&lt;00:10, 22.00it/s] 78%|███████▊  | 783/1000 [00:41&lt;00:11, 18.32it/s] 79%|███████▊  | 786/1000 [00:42&lt;00:11, 17.96it/s] 79%|███████▉  | 793/1000 [00:42&lt;00:11, 18.17it/s] 80%|████████  | 800/1000 [00:42&lt;00:09, 21.84it/s] 80%|████████  | 805/1000 [00:43&lt;00:14, 13.73it/s] 81%|████████▏ | 814/1000 [00:43&lt;00:09, 18.86it/s] 82%|████████▏ | 821/1000 [00:44&lt;00:09, 18.74it/s] 83%|████████▎ | 827/1000 [00:44&lt;00:08, 20.19it/s] 83%|████████▎ | 833/1000 [00:44&lt;00:08, 19.55it/s] 84%|████████▍ | 839/1000 [00:44&lt;00:06, 23.78it/s] 84%|████████▍ | 843/1000 [00:45&lt;00:07, 20.32it/s] 85%|████████▍ | 847/1000 [00:45&lt;00:07, 19.19it/s] 85%|████████▌ | 851/1000 [00:45&lt;00:07, 20.65it/s] 86%|████████▌ | 857/1000 [00:45&lt;00:08, 16.37it/s] 86%|████████▋ | 864/1000 [00:46&lt;00:06, 21.56it/s] 87%|████████▋ | 867/1000 [00:46&lt;00:10, 13.14it/s] 87%|████████▋ | 871/1000 [00:46&lt;00:08, 14.97it/s] 88%|████████▊ | 878/1000 [00:47&lt;00:07, 17.27it/s] 89%|████████▊ | 887/1000 [00:47&lt;00:04, 25.31it/s] 89%|████████▉ | 891/1000 [00:47&lt;00:04, 26.92it/s] 90%|████████▉ | 895/1000 [00:47&lt;00:06, 17.31it/s] 90%|█████████ | 900/1000 [00:48&lt;00:07, 13.38it/s] 91%|█████████ | 910/1000 [00:48&lt;00:04, 21.95it/s] 92%|█████████▏| 915/1000 [00:48&lt;00:03, 24.15it/s] 92%|█████████▏| 920/1000 [00:49&lt;00:03, 21.04it/s] 92%|█████████▏| 924/1000 [00:49&lt;00:04, 17.22it/s] 93%|█████████▎| 928/1000 [00:49&lt;00:03, 18.54it/s] 94%|█████████▎| 935/1000 [00:49&lt;00:03, 21.63it/s] 94%|█████████▍| 938/1000 [00:50&lt;00:03, 16.73it/s] 94%|█████████▍| 941/1000 [00:50&lt;00:03, 15.44it/s] 94%|█████████▍| 944/1000 [00:50&lt;00:03, 15.71it/s] 95%|█████████▌| 952/1000 [00:50&lt;00:02, 20.54it/s] 96%|█████████▌| 955/1000 [00:51&lt;00:03, 14.43it/s] 96%|█████████▌| 962/1000 [00:51&lt;00:01, 19.87it/s] 96%|█████████▋| 965/1000 [00:51&lt;00:01, 20.81it/s] 97%|█████████▋| 969/1000 [00:51&lt;00:01, 22.25it/s] 97%|█████████▋| 972/1000 [00:52&lt;00:01, 15.39it/s] 98%|█████████▊| 982/1000 [00:52&lt;00:00, 22.65it/s] 98%|█████████▊| 985/1000 [00:52&lt;00:00, 18.23it/s] 99%|█████████▉| 988/1000 [00:52&lt;00:00, 18.29it/s] 99%|█████████▉| 991/1000 [00:53&lt;00:00, 12.99it/s]100%|█████████▉| 995/1000 [00:53&lt;00:00, 15.73it/s]100%|██████████| 1000/1000 [00:53&lt;00:00, 18.71it/s]\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n                                                  Review  \\\n10957  worst exp, just got dissapointed, hotel midle ...   \n6644   great, ok place gets reviews run gamut right, ...   \n12619  did n't maid n't want re-write positive review...   \n15367  resort personality simpicity charm just got ba...   \n4290   wonderful stay, stayed hotel new year holiday ...   \n\n                                       aspect_sentiments  \n10957  {'location': {'label': 'Not Mentioned', 'score...  \n6644   {'location': {'label': 'Not Mentioned', 'score...  \n12619  {'location': {'label': 'NEGATIVE', 'score': -0...  \n15367  {'location': {'label': 'Not Mentioned', 'score...  \n4290   {'location': {'label': 'POSITIVE', 'score': 0....  \n\n\n\n\nCode\n# Count occurrences of each label for each aspect\nlabel_columns = [col for col in aspect_scores_df.columns if '.label' in col]\nlabel_counts = {}\n\nfor col in label_columns:\n    aspect = col.split('.')[0]\n    label_counts[aspect] = aspect_scores_df[col].value_counts()\n\n# Convert to DataFrame\nlabel_counts_df = pd.DataFrame(label_counts).fillna(0)\n\n# Plot a bar chart\nlabel_counts_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')\nplt.title(\"Sentiment Distribution Across Aspects\")\nplt.xlabel(\"Sentiment Labels\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.legend(title=\"Aspects\", loc='upper right')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean scores for each aspect\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\n\n# Prepare heatmap data\nheatmap_data = pd.DataFrame(mean_scores).T\nheatmap_data.columns = [col.split('.')[0] for col in heatmap_data.columns]\n\n# Plot heatmap\nplt.figure(figsize=(10, 4))\nsns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\nplt.title(\"Mean Sentiment Scores for Aspects\")\nplt.xlabel(\"Aspects\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Calculate mean sentiment scores\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\naspects = [col.split('.')[0] for col in mean_scores.index]\n\n# Radar chart\nfig = go.Figure()\nfig.add_trace(go.Scatterpolar(\n    r=mean_scores,\n    theta=aspects,\n    fill='toself',\n    name='Average Sentiment Scores'\n))\n\nfig.update_layout(\n    polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),\n    title=\"Aspect-Based Sentiment Radar Chart\",\n    showlegend=True\n)\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.express as px\n\n# Prepare data for the sunburst chart\nlabel_data = []\nfor aspect in ['location', 'service', 'room', 'staff']:\n    labels = aspect_scores_df[f'{aspect}.label'].value_counts()\n    for label, count in labels.items():\n        label_data.append({'Aspect': aspect, 'Label': label, 'Count': count})\n\nlabel_df = pd.DataFrame(label_data)\n\n# Sunburst chart\nfig = px.sunburst(label_df, path=['Aspect', 'Label'], values='Count',\n                  title=\"Aspect-Based Sentiment Distribution (Sunburst)\",\n                  color='Count', color_continuous_scale='Viridis')\nfig.show()"
  },
  {
    "objectID": "index.html#understanding-guest-experiences-sentiment-and-n-gram-analysis-of-tripadvisor-hotel-reviews",
    "href": "index.html#understanding-guest-experiences-sentiment-and-n-gram-analysis-of-tripadvisor-hotel-reviews",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "text": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews\nThis project analyzes customer feedback through aspect-based sentiment analysis and n-gram analysis, providing actionable insights into guest experiences. Key aspects like location, service, room, and staff were evaluated to identify strengths, weaknesses, and trends in reviews.\n\nKey Methods\n\nAspect-Based Sentiment Analysis: Using Hugging Face’s DistilBERT, I classified sentiments for each aspect, supported by multithreading for scalability.\nTerm Frequency and N-gram Analysis: Common words, bigrams, and trigrams were identified, normalized, and visualized to uncover recurring themes and guest priorities.\n\n\n\nWhy This Matters\nBy combining sentiment and linguistic analyses, this approach enables businesses to: - Understand guest preferences and concerns. - Improve service quality with data-driven insights. - Uncover patterns in customer feedback to enhance guest satisfaction."
  },
  {
    "objectID": "index.html#aspect-based-sentiment-analysis",
    "href": "index.html#aspect-based-sentiment-analysis",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Aspect-Based Sentiment Analysis",
    "text": "Aspect-Based Sentiment Analysis\nI implemented an aspect-based sentiment analysis to uncover insights from hotel reviews, focusing on key aspects: location, service, room, and staff. This analysis helps break down overall feedback into actionable insights for each category.\n\nKey Steps\n\nData Preprocessing: Cleaned and prepared reviews for analysis to ensure consistency.\nSentiment Classification: Used Hugging Face’s DistilBERT, a cutting-edge machine learning model, to classify sentiments as positive or negative for each aspect.\nAspect-Specific Analysis:\n\nFor each review, I generated prompts like, “What do you think about the service?” to extract aspect-focused sentiment.\nIf an aspect wasn’t mentioned, it was marked as ‘Not Mentioned’.\n\n\n\n\nEfficiency at Scale\nTo analyze thousands of reviews efficiently, I employed multithreading, speeding up the process while maintaining accuracy.\n\n\nData Visualization\nI structured the results into interactive visualizations, including: - Sentiment Scores: Displaying the average sentiment for each aspect. - Radar Charts: Providing an intuitive overview of customer perceptions.\n\n\nCode\n## Define aspects\naspects = ['location', 'service', 'room', 'staff']\n\nimport pandas as pd\nfrom transformers import pipeline\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\ndf_sample = df.sample(1000, random_state=42) \ndf_sample['cleaned_review'] = df_sample['Review'].apply(clean_text)\n\naspect_sentiment_pipeline = pipeline(\n    \"text-classification\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    framework=\"pt\",  # Force PyTorch\n    truncation=True,\n    padding=True\n)\n\n# Function to handle missing aspects and perform sentiment analysis\ndef analyze_aspect_sentiments_with_missing_handling(review, aspects):\n    aspect_sentiments = {}\n    for aspect in aspects:\n        # Check if the aspect is mentioned in the review\n        if aspect in review:\n            text = f\"{review} What do you think about the {aspect}?\"\n            result = aspect_sentiment_pipeline(text)\n            aspect_sentiments[aspect] = {\n                'label': result[0]['label'],  # Positive, Neutral, or Negative\n                'score': result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']\n            }\n        else:\n            # Assign default value if aspect is not mentioned\n            aspect_sentiments[aspect] = {\n                'label': 'Not Mentioned',\n                'score': None\n            }\n    return aspect_sentiments\n# Apply the function using multithreading for efficiency\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(tqdm(executor.map(lambda x: analyze_aspect_sentiments_with_missing_handling(x, aspects), \n                                     df_sample['cleaned_review']), \n                        total=len(df_sample)))\n\n# Add results to the DataFrame\ndf_sample['aspect_sentiments'] = results\n\n# Extract aspect scores into a separate DataFrame\naspect_scores_df = pd.json_normalize(df_sample['aspect_sentiments'])\n\nprint(df_sample[['Review', 'aspect_sentiments']].head())\n\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  0%|          | 1/1000 [00:00&lt;09:28,  1.76it/s]  0%|          | 2/1000 [00:00&lt;05:28,  3.04it/s]  0%|          | 3/1000 [00:01&lt;05:09,  3.22it/s]  0%|          | 4/1000 [00:01&lt;04:29,  3.70it/s]  1%|          | 12/1000 [00:01&lt;01:08, 14.43it/s]  1%|▏         | 14/1000 [00:01&lt;01:21, 12.10it/s]  2%|▏         | 17/1000 [00:02&lt;01:32, 10.60it/s]  2%|▏         | 21/1000 [00:02&lt;01:09, 14.09it/s]  3%|▎         | 28/1000 [00:02&lt;00:45, 21.41it/s]  3%|▎         | 31/1000 [00:02&lt;00:57, 16.94it/s]  4%|▎         | 37/1000 [00:03&lt;01:31, 10.57it/s]  5%|▍         | 49/1000 [00:03&lt;00:50, 18.77it/s]  6%|▌         | 56/1000 [00:04&lt;00:59, 15.97it/s]  7%|▋         | 68/1000 [00:04&lt;00:38, 24.07it/s]  7%|▋         | 72/1000 [00:04&lt;00:39, 23.39it/s]  8%|▊         | 76/1000 [00:05&lt;00:54, 17.02it/s]  9%|▊         | 86/1000 [00:05&lt;00:35, 25.46it/s]  9%|▉         | 91/1000 [00:05&lt;00:40, 22.43it/s] 10%|█         | 100/1000 [00:05&lt;00:34, 25.89it/s] 10%|█         | 104/1000 [00:06&lt;00:36, 24.55it/s] 11%|█         | 108/1000 [00:06&lt;00:41, 21.25it/s] 11%|█         | 111/1000 [00:07&lt;01:19, 11.19it/s] 12%|█▎        | 125/1000 [00:07&lt;00:39, 22.19it/s] 13%|█▎        | 130/1000 [00:07&lt;00:42, 20.40it/s] 13%|█▎        | 134/1000 [00:07&lt;00:43, 19.71it/s] 14%|█▍        | 138/1000 [00:08&lt;00:56, 15.25it/s] 15%|█▌        | 150/1000 [00:08&lt;00:40, 21.15it/s] 15%|█▌        | 154/1000 [00:08&lt;00:38, 22.05it/s] 16%|█▌        | 160/1000 [00:08&lt;00:34, 24.51it/s] 16%|█▋        | 164/1000 [00:09&lt;00:31, 26.64it/s] 17%|█▋        | 168/1000 [00:09&lt;00:41, 20.16it/s] 17%|█▋        | 171/1000 [00:09&lt;01:01, 13.39it/s] 18%|█▊        | 183/1000 [00:10&lt;00:37, 21.71it/s] 19%|█▊        | 186/1000 [00:10&lt;00:40, 19.96it/s] 19%|█▉        | 189/1000 [00:10&lt;00:40, 19.95it/s] 19%|█▉        | 192/1000 [00:10&lt;00:42, 19.08it/s] 20%|█▉        | 195/1000 [00:10&lt;00:45, 17.63it/s] 20%|█▉        | 197/1000 [00:11&lt;00:45, 17.65it/s] 20%|██        | 200/1000 [00:11&lt;01:08, 11.75it/s] 20%|██        | 202/1000 [00:12&lt;01:42,  7.82it/s] 22%|██▏       | 217/1000 [00:12&lt;00:42, 18.28it/s] 22%|██▏       | 222/1000 [00:12&lt;00:39, 19.95it/s] 22%|██▎       | 225/1000 [00:12&lt;00:38, 19.91it/s] 23%|██▎       | 228/1000 [00:13&lt;00:44, 17.34it/s] 23%|██▎       | 230/1000 [00:13&lt;00:43, 17.67it/s] 23%|██▎       | 234/1000 [00:13&lt;00:36, 20.97it/s] 24%|██▎       | 237/1000 [00:13&lt;00:45, 16.60it/s] 24%|██▍       | 242/1000 [00:13&lt;00:50, 15.16it/s] 25%|██▌       | 254/1000 [00:14&lt;00:28, 25.96it/s] 26%|██▌       | 258/1000 [00:14&lt;00:26, 27.67it/s] 26%|██▌       | 262/1000 [00:14&lt;00:48, 15.19it/s] 27%|██▋       | 271/1000 [00:15&lt;00:43, 16.62it/s] 28%|██▊       | 278/1000 [00:15&lt;00:34, 20.67it/s] 28%|██▊       | 283/1000 [00:15&lt;00:42, 16.97it/s] 29%|██▉       | 293/1000 [00:16&lt;00:40, 17.46it/s] 30%|██▉       | 296/1000 [00:16&lt;00:40, 17.49it/s] 30%|███       | 305/1000 [00:16&lt;00:29, 23.66it/s] 31%|███       | 309/1000 [00:17&lt;00:31, 21.74it/s] 31%|███       | 312/1000 [00:17&lt;00:53, 12.80it/s] 32%|███▏      | 324/1000 [00:18&lt;00:35, 19.29it/s] 33%|███▎      | 329/1000 [00:18&lt;00:30, 22.14it/s] 33%|███▎      | 333/1000 [00:18&lt;00:28, 23.57it/s] 34%|███▎      | 337/1000 [00:18&lt;00:30, 21.40it/s] 34%|███▍      | 341/1000 [00:18&lt;00:30, 21.56it/s] 34%|███▍      | 345/1000 [00:19&lt;00:34, 18.90it/s] 35%|███▌      | 352/1000 [00:19&lt;00:39, 16.34it/s] 36%|███▌      | 358/1000 [00:19&lt;00:37, 17.35it/s] 36%|███▋      | 365/1000 [00:19&lt;00:27, 23.50it/s] 37%|███▋      | 369/1000 [00:20&lt;00:29, 21.09it/s] 37%|███▋      | 372/1000 [00:20&lt;00:29, 21.28it/s] 38%|███▊      | 376/1000 [00:20&lt;00:27, 22.80it/s] 38%|███▊      | 380/1000 [00:20&lt;00:33, 18.37it/s] 38%|███▊      | 383/1000 [00:20&lt;00:31, 19.88it/s] 39%|███▉      | 388/1000 [00:21&lt;00:53, 11.45it/s] 40%|███▉      | 397/1000 [00:22&lt;00:40, 14.92it/s] 40%|████      | 402/1000 [00:22&lt;00:35, 16.94it/s] 41%|████      | 407/1000 [00:22&lt;00:28, 20.50it/s] 41%|████      | 410/1000 [00:22&lt;00:31, 18.62it/s] 41%|████▏     | 413/1000 [00:22&lt;00:37, 15.46it/s] 42%|████▏     | 420/1000 [00:23&lt;00:29, 19.57it/s] 42%|████▎     | 425/1000 [00:23&lt;00:33, 17.00it/s] 43%|████▎     | 432/1000 [00:24&lt;00:33, 16.94it/s] 43%|████▎     | 434/1000 [00:24&lt;00:33, 16.99it/s] 44%|████▎     | 436/1000 [00:24&lt;00:35, 15.93it/s] 44%|████▍     | 441/1000 [00:24&lt;00:30, 18.25it/s] 44%|████▍     | 444/1000 [00:24&lt;00:33, 16.60it/s] 45%|████▍     | 446/1000 [00:25&lt;00:40, 13.53it/s] 45%|████▌     | 452/1000 [00:25&lt;00:27, 19.62it/s] 46%|████▌     | 455/1000 [00:25&lt;00:38, 14.05it/s] 46%|████▌     | 461/1000 [00:26&lt;00:43, 12.50it/s] 47%|████▋     | 466/1000 [00:26&lt;00:41, 12.95it/s] 47%|████▋     | 474/1000 [00:27&lt;00:38, 13.60it/s] 48%|████▊     | 481/1000 [00:27&lt;00:27, 18.57it/s] 48%|████▊     | 484/1000 [00:27&lt;00:25, 19.89it/s] 49%|████▊     | 487/1000 [00:27&lt;00:32, 15.83it/s] 49%|████▉     | 490/1000 [00:27&lt;00:40, 12.51it/s] 50%|████▉     | 498/1000 [00:28&lt;00:43, 11.54it/s] 51%|█████     | 510/1000 [00:29&lt;00:26, 18.19it/s] 52%|█████▏    | 517/1000 [00:29&lt;00:23, 20.77it/s] 52%|█████▏    | 522/1000 [00:29&lt;00:25, 18.94it/s] 52%|█████▎    | 525/1000 [00:29&lt;00:23, 19.92it/s] 53%|█████▎    | 529/1000 [00:29&lt;00:25, 18.21it/s] 53%|█████▎    | 533/1000 [00:30&lt;00:22, 20.85it/s] 54%|█████▎    | 536/1000 [00:30&lt;00:35, 12.89it/s] 54%|█████▍    | 544/1000 [00:30&lt;00:22, 20.46it/s] 55%|█████▍    | 548/1000 [00:30&lt;00:22, 20.23it/s] 55%|█████▌    | 552/1000 [00:31&lt;00:20, 21.58it/s] 56%|█████▌    | 555/1000 [00:31&lt;00:19, 22.28it/s] 56%|█████▌    | 558/1000 [00:31&lt;00:19, 22.11it/s] 56%|█████▋    | 564/1000 [00:31&lt;00:19, 22.32it/s] 57%|█████▋    | 567/1000 [00:31&lt;00:25, 17.15it/s] 57%|█████▋    | 574/1000 [00:32&lt;00:18, 23.04it/s] 58%|█████▊    | 577/1000 [00:32&lt;00:19, 22.19it/s] 58%|█████▊    | 581/1000 [00:32&lt;00:21, 19.60it/s] 59%|█████▉    | 588/1000 [00:33&lt;00:33, 12.22it/s] 61%|██████    | 606/1000 [00:33&lt;00:15, 24.80it/s] 61%|██████    | 612/1000 [00:33&lt;00:17, 22.36it/s] 62%|██████▏   | 620/1000 [00:34&lt;00:16, 22.36it/s] 62%|██████▎   | 625/1000 [00:34&lt;00:19, 19.50it/s] 63%|██████▎   | 633/1000 [00:34&lt;00:14, 25.61it/s] 64%|██████▎   | 637/1000 [00:35&lt;00:14, 24.82it/s] 64%|██████▍   | 641/1000 [00:35&lt;00:16, 22.13it/s] 65%|██████▍   | 648/1000 [00:35&lt;00:14, 24.24it/s] 65%|██████▌   | 653/1000 [00:36&lt;00:23, 14.93it/s] 67%|██████▋   | 666/1000 [00:36&lt;00:13, 25.02it/s] 67%|██████▋   | 671/1000 [00:36&lt;00:13, 25.14it/s] 68%|██████▊   | 675/1000 [00:37&lt;00:18, 17.12it/s] 68%|██████▊   | 682/1000 [00:37&lt;00:16, 19.80it/s] 68%|██████▊   | 685/1000 [00:37&lt;00:16, 19.57it/s] 69%|██████▉   | 688/1000 [00:37&lt;00:17, 18.28it/s] 69%|██████▉   | 692/1000 [00:37&lt;00:15, 19.70it/s] 70%|██████▉   | 695/1000 [00:38&lt;00:15, 19.42it/s] 70%|██████▉   | 698/1000 [00:38&lt;00:16, 18.33it/s] 70%|███████   | 700/1000 [00:38&lt;00:26, 11.46it/s] 70%|███████   | 704/1000 [00:38&lt;00:21, 14.06it/s] 71%|███████▏  | 713/1000 [00:39&lt;00:13, 20.60it/s] 72%|███████▏  | 719/1000 [00:39&lt;00:12, 21.84it/s] 72%|███████▏  | 722/1000 [00:39&lt;00:18, 14.64it/s] 73%|███████▎  | 729/1000 [00:39&lt;00:13, 20.15it/s] 74%|███████▎  | 735/1000 [00:40&lt;00:11, 23.62it/s] 74%|███████▍  | 739/1000 [00:40&lt;00:19, 13.25it/s] 76%|███████▌  | 755/1000 [00:40&lt;00:08, 27.72it/s] 76%|███████▌  | 761/1000 [00:41&lt;00:09, 24.28it/s] 77%|███████▋  | 766/1000 [00:41&lt;00:10, 23.25it/s] 77%|███████▋  | 770/1000 [00:42&lt;00:14, 16.25it/s] 78%|███████▊  | 778/1000 [00:42&lt;00:09, 22.35it/s] 78%|███████▊  | 783/1000 [00:42&lt;00:12, 17.41it/s] 79%|███████▉  | 790/1000 [00:42&lt;00:09, 22.52it/s] 79%|███████▉  | 794/1000 [00:43&lt;00:09, 21.68it/s] 80%|████████  | 800/1000 [00:43&lt;00:10, 19.56it/s] 80%|████████  | 805/1000 [00:44&lt;00:16, 11.60it/s] 82%|████████▏ | 821/1000 [00:44&lt;00:07, 22.88it/s] 83%|████████▎ | 826/1000 [00:44&lt;00:07, 24.26it/s] 83%|████████▎ | 831/1000 [00:45&lt;00:08, 20.49it/s] 84%|████████▎ | 835/1000 [00:45&lt;00:07, 22.16it/s] 84%|████████▍ | 839/1000 [00:45&lt;00:07, 21.54it/s] 84%|████████▍ | 842/1000 [00:45&lt;00:09, 16.46it/s] 85%|████████▍ | 846/1000 [00:45&lt;00:08, 19.00it/s] 85%|████████▍ | 849/1000 [00:46&lt;00:09, 16.54it/s] 86%|████████▌ | 855/1000 [00:46&lt;00:06, 21.36it/s] 86%|████████▌ | 858/1000 [00:46&lt;00:11, 12.50it/s] 87%|████████▋ | 866/1000 [00:47&lt;00:08, 16.10it/s] 87%|████████▋ | 871/1000 [00:47&lt;00:09, 12.99it/s] 88%|████████▊ | 880/1000 [00:47&lt;00:05, 20.01it/s] 88%|████████▊ | 884/1000 [00:48&lt;00:05, 19.42it/s] 89%|████████▉ | 889/1000 [00:48&lt;00:04, 23.01it/s] 89%|████████▉ | 893/1000 [00:48&lt;00:05, 17.88it/s] 90%|████████▉ | 896/1000 [00:48&lt;00:05, 17.53it/s] 90%|█████████ | 900/1000 [00:48&lt;00:05, 17.82it/s] 91%|█████████ | 907/1000 [00:49&lt;00:04, 22.72it/s] 91%|█████████ | 911/1000 [00:49&lt;00:03, 24.26it/s] 92%|█████████▏| 916/1000 [00:49&lt;00:03, 26.12it/s] 92%|█████████▏| 919/1000 [00:49&lt;00:03, 23.42it/s] 92%|█████████▏| 923/1000 [00:50&lt;00:05, 13.05it/s] 94%|█████████▎| 935/1000 [00:50&lt;00:03, 19.38it/s] 94%|█████████▍| 938/1000 [00:50&lt;00:03, 19.14it/s] 94%|█████████▍| 941/1000 [00:51&lt;00:04, 13.13it/s] 95%|█████████▌| 953/1000 [00:51&lt;00:02, 16.06it/s] 96%|█████████▋| 963/1000 [00:52&lt;00:01, 21.89it/s] 97%|█████████▋| 970/1000 [00:52&lt;00:01, 26.52it/s] 97%|█████████▋| 974/1000 [00:52&lt;00:01, 22.28it/s] 98%|█████████▊| 981/1000 [00:52&lt;00:00, 27.89it/s] 98%|█████████▊| 985/1000 [00:53&lt;00:01, 14.09it/s] 99%|█████████▉| 991/1000 [00:53&lt;00:00, 15.46it/s]100%|█████████▉| 995/1000 [00:54&lt;00:00, 15.23it/s]100%|██████████| 1000/1000 [00:54&lt;00:00, 18.49it/s]\n\n\n                                                  Review  \\\n10957  worst exp, just got dissapointed, hotel midle ...   \n6644   great, ok place gets reviews run gamut right, ...   \n12619  did n't maid n't want re-write positive review...   \n15367  resort personality simpicity charm just got ba...   \n4290   wonderful stay, stayed hotel new year holiday ...   \n\n                                       aspect_sentiments  \n10957  {'location': {'label': 'Not Mentioned', 'score...  \n6644   {'location': {'label': 'Not Mentioned', 'score...  \n12619  {'location': {'label': 'NEGATIVE', 'score': -0...  \n15367  {'location': {'label': 'Not Mentioned', 'score...  \n4290   {'location': {'label': 'POSITIVE', 'score': 0....  \n\n\n\n\nCode\n# Count occurrences of each label for each aspect\nlabel_columns = [col for col in aspect_scores_df.columns if '.label' in col]\nlabel_counts = {}\n\nfor col in label_columns:\n    aspect = col.split('.')[0]\n    label_counts[aspect] = aspect_scores_df[col].value_counts()\n\n# Convert to DataFrame\nlabel_counts_df = pd.DataFrame(label_counts).fillna(0)\n\n# Plot a bar chart\nlabel_counts_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')\nplt.title(\"Sentiment Distribution Across Aspects\")\nplt.xlabel(\"Sentiment Labels\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.legend(title=\"Aspects\", loc='upper right')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean scores for each aspect\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\n\n# Prepare heatmap data\nheatmap_data = pd.DataFrame(mean_scores).T\nheatmap_data.columns = [col.split('.')[0] for col in heatmap_data.columns]\n\n# Plot heatmap\nplt.figure(figsize=(10, 4))\nsns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\nplt.title(\"Mean Sentiment Scores for Aspects\")\nplt.xlabel(\"Aspects\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Calculate mean sentiment scores\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\naspects = [col.split('.')[0] for col in mean_scores.index]\n\n# Radar chart\nfig = go.Figure()\nfig.add_trace(go.Scatterpolar(\n    r=mean_scores,\n    theta=aspects,\n    fill='toself',\n    name='Average Sentiment Scores'\n))\n\nfig.update_layout(\n    polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),\n    title=\"Aspect-Based Sentiment Radar Chart\",\n    showlegend=True\n)\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.express as px\n\n# Prepare data for the sunburst chart\nlabel_data = []\nfor aspect in ['location', 'service', 'room', 'staff']:\n    labels = aspect_scores_df[f'{aspect}.label'].value_counts()\n    for label, count in labels.items():\n        label_data.append({'Aspect': aspect, 'Label': label, 'Count': count})\n\nlabel_df = pd.DataFrame(label_data)\n\n# Sunburst chart\nfig = px.sunburst(label_df, path=['Aspect', 'Label'], values='Count',\n                  title=\"Aspect-Based Sentiment Distribution (Sunburst)\",\n                  color='Count', color_continuous_scale='Viridis')\nfig.show()"
  },
  {
    "objectID": "index.html#term-frequency-and-n-gram-analysis",
    "href": "index.html#term-frequency-and-n-gram-analysis",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Term Frequency and N-gram Analysis",
    "text": "Term Frequency and N-gram Analysis\nIn addition to sentiment analysis, I explored the linguistic patterns in guest reviews through term frequency and n-gram analysis (bigrams and trigrams). These techniques highlight recurring phrases that reflect customer priorities and recurring themes.\n\nKey Steps\n\nTerm Frequency:\n\n\nTokenized and cleaned text data to calculate the frequency of individual words.\nIdentified the most common words, excluding generic stopwords, to reveal dominant topics like “service,” “clean,” and “location.”\n\n\nN-gram Analysis:\n\n\nExtracted bigrams (two-word combinations) and trigrams (three-word combinations) to capture contextual patterns.\nNormalized n-grams (e.g., treating “great hotel” and “hotel great” as identical) for more meaningful insights.\nApplied manual filters to exclude irrelevant or uninformative n-grams (e.g., “ca nt,” “did not”).\n\n\n\nCode\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport nltk\n\n# Step 1: Download Stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Function to clean text (if not already done)\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    return text\n\n# Assuming 'df' contains the 'Review' column\n# Preprocess reviews\ndf['cleaned_review'] = df['Review'].apply(clean_text)\n\n# Function to generate n-grams from text\ndef generate_ngrams(text, n):\n    tokens = text.split()  # Split text into words\n    return list(ngrams(tokens, n))  # Generate n-grams\n\n# Step 2: Generate bigrams and trigrams\ndf['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))  # Bigrams\ndf['trigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 3))  # Trigrams\n\n# Step 3: Flatten bigrams and trigrams into lists\nall_bigrams = [bigram for bigram_list in df['bigrams'] for bigram in bigram_list]\nall_trigrams = [trigram for trigram_list in df['trigrams'] for trigram in trigram_list]\n\n# Step 4: Normalize bigrams and trigrams (sort alphabetically to handle duplicates)\ndef normalize_ngram(ngram):\n    return tuple(sorted(ngram))  # Sort words alphabetically in each n-gram\n\nnormalized_bigrams = [normalize_ngram(bigram) for bigram in all_bigrams]\nnormalized_trigrams = [normalize_ngram(trigram) for trigram in all_trigrams]\n\n# Step 5: Count frequencies of normalized bigrams and trigrams\nbigram_counts = Counter(normalized_bigrams)\ntrigram_counts = Counter(normalized_trigrams)\n\n# Step 6: Exclude unwanted bigrams\nexclude_bigrams = [('ca', 'nt'), ('did', 'nt'), ('did', 'not')]\n\ndef filter_exclude_bigrams(bigram_counts, exclude_list):\n    filtered_bigrams = {bigram: count for bigram, count in bigram_counts.items() if bigram not in exclude_list}\n    return filtered_bigrams\n\nfiltered_bigram_counts = filter_exclude_bigrams(bigram_counts, exclude_bigrams)\n\n# Step 7: Get the most common bigrams and trigrams\nmost_common_filtered_bigrams = Counter(filtered_bigram_counts).most_common(20)\nmost_common_trigrams = trigram_counts.most_common(20)\n\n# Step 8: Convert results to DataFrames for visualization\nfiltered_bigram_df = pd.DataFrame(most_common_filtered_bigrams, columns=['Bigram', 'Frequency'])\nfiltered_bigram_df['Bigram'] = filtered_bigram_df['Bigram'].apply(lambda x: ' '.join(x))  # Convert tuples to strings\n\ntrigram_df = pd.DataFrame(most_common_trigrams, columns=['Trigram', 'Frequency'])\ntrigram_df['Trigram'] = trigram_df['Trigram'].apply(lambda x: ' '.join(x))  # Convert tuples to strings\n\n# Step 9: Visualization for Bigrams\ncolors = plt.cm.viridis(np.linspace(0, 1, len(filtered_bigram_df)))  # Generate colors from a colormap\n\nplt.figure(figsize=(10, 6))\nplt.barh(filtered_bigram_df['Bigram'], filtered_bigram_df['Frequency'], color=colors)\nplt.title('Top 20 Filtered Bigrams in Reviews', fontsize=16)\nplt.xlabel('Frequency', fontsize=12)\nplt.ylabel('Bigrams', fontsize=12)\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n# Step 10: Visualization for Trigrams\ncolors = plt.cm.plasma(np.linspace(0, 1, len(trigram_df)))  # Use a different colormap for trigrams\n\nplt.figure(figsize=(10, 6))\nplt.barh(trigram_df['Trigram'], trigram_df['Frequency'], color=colors)\nplt.title('Top 20 Trigrams in Reviews', fontsize=16)\nplt.xlabel('Frequency', fontsize=12)\nplt.ylabel('Trigrams', fontsize=12)\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\n[nltk_data] Downloading package stopwords to /Users/yixin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  }
]
