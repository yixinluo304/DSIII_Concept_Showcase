[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "",
    "text": "Yixin Luo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#eda",
    "href": "index.html#eda",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\n\n## Load data\ndf = pd.read_csv('~/Downloads/tripadvisor_hotel_reviews.csv')\n\n\n## distribution of ratings\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x='Rating', palette='viridis')\nplt.title('Distribution of Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nCode\n## Check the length of reviews\ndf[\"word_length\"] = df[\"Review\"].apply(len)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"word_length\", hue=\"Rating\", multiple=\"stack\", palette=\"bright\")\nplt.title('Distribution of Text Lengths by Class', fontsize=16)\nplt.xlabel('Text Length', fontsize=14)\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom collections import Counter\nimport pandas as pd\nimport re\n\n# Assuming your DataFrame is named 'df' and the column with text is 'Review'\n\n# Step 1: Preprocess the text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    return text\n\ndf['cleaned_review'] = df['Review'].apply(clean_text)\n\n# Step 2: Tokenize the text and count words\nall_words = ' '.join(df['cleaned_review']).split()  # Combine all reviews and split into words\nword_counts = Counter(all_words)  # Count word frequencies\n\n# Step 3: Get the most common words\nmost_common_words = word_counts.most_common(20)  # Top 20 most common words\nprint(\"Most Common Words:\", most_common_words)\n\n# Convert to a DataFrame for better readability\nmost_common_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\nprint(most_common_df)\n\n# Horizontal visualization\nplt.figure(figsize=(10, 6))\nplt.barh(most_common_df['Word'], most_common_df['Frequency'], color='skyblue')\nplt.title('Top 20 Most Common Words in Reviews', fontsize=16)\nplt.xlabel('Frequency', fontsize=12)\nplt.ylabel('Words', fontsize=12)\nplt.gca().invert_yaxis()  # Invert y-axis to show highest frequency on top\nplt.tight_layout()\nplt.show()\n\n\nMost Common Words: [('hotel', 48864), ('room', 34324), ('not', 31525), ('great', 21094), ('nt', 19000), ('good', 16986), ('staff', 16213), ('stay', 15158), ('did', 13895), ('just', 12592), ('nice', 12409), ('rooms', 12024), ('no', 11620), ('location', 11043), ('stayed', 10469), ('service', 9975), ('time', 9824), ('night', 9728), ('beach', 9592), ('day', 9541)]\n        Word  Frequency\n0      hotel      48864\n1       room      34324\n2        not      31525\n3      great      21094\n4         nt      19000\n5       good      16986\n6      staff      16213\n7       stay      15158\n8        did      13895\n9       just      12592\n10      nice      12409\n11     rooms      12024\n12        no      11620\n13  location      11043\n14    stayed      10469\n15   service       9975\n16      time       9824\n17     night       9728\n18     beach       9592\n19       day       9541"
  },
  {
    "objectID": "index.html#text-cleaning",
    "href": "index.html#text-cleaning",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Text Cleaning",
    "text": "Text Cleaning\n\nremove the big comments\n\n\nCode\n# remove the big comments\ndf = df[df['word_length'] &lt;= 3000]\n\ndf[\"word_length\"] = df[\"Review\"].apply(len)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"word_length\", hue=\"Rating\", multiple=\"stack\", palette=\"bright\")\nplt.title('Distribution of Text Lengths by Class', fontsize=16)\nplt.xlabel('Text Length', fontsize=14)\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n\n# Text cleaning function\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n    return text\n\n# Apply text cleaning\ndf.loc[:, 'cleaned_review'] = df['Review'].apply(clean_text)\nprint(df)\n\n\n                                                  Review  Rating  word_length  \\\n0      nice hotel expensive parking got good deal sta...       4          593   \n1      ok nothing special charge diamond member hilto...       2         1689   \n2      nice rooms not 4* experience hotel monaco seat...       3         1427   \n3      unique, great stay, wonderful time hotel monac...       5          600   \n4      great stay great stay, went seahawk game aweso...       5         1281   \n...                                                  ...     ...          ...   \n20485  not impressed unfriendly staff checked asked h...       2          616   \n20486  best kept secret 3rd time staying charm, not 5...       5          733   \n20487  great location price view hotel great quick pl...       4          306   \n20488  ok just looks nice modern outside, desk staff ...       2          443   \n20490  people talking, ca n't believe excellent ratin...       2          620   \n\n                                          cleaned_review  \n0      nice hotel expensive parking got good deal sta...  \n1      ok nothing special charge diamond member hilto...  \n2      nice rooms not experience hotel monaco seattle...  \n3      unique great stay wonderful time hotel monaco ...  \n4      great stay great stay went seahawk game awesom...  \n...                                                  ...  \n20485  not impressed unfriendly staff checked asked h...  \n20486  best kept secret rd time staying charm not sta...  \n20487  great location price view hotel great quick pl...  \n20488  ok just looks nice modern outside desk staff n...  \n20490  people talking ca nt believe excellent ratings...  \n\n[20196 rows x 4 columns]"
  },
  {
    "objectID": "index.html#sentiment-analysis",
    "href": "index.html#sentiment-analysis",
    "title": "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\nCode\n## Define aspects\naspects = ['location', 'service', 'room', 'staff']\n\nimport pandas as pd\nfrom transformers import pipeline\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\ndf_sample = df.sample(1000, random_state=42) \ndf_sample['cleaned_review'] = df_sample['Review'].apply(clean_text)\n\n# Initialize DistilBERT pipeline\naspect_sentiment_pipeline = pipeline(\n    \"text-classification\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    truncation=True,\n    padding=True\n)\n\n# Function to handle missing aspects and perform sentiment analysis\ndef analyze_aspect_sentiments_with_missing_handling(review, aspects):\n    aspect_sentiments = {}\n    for aspect in aspects:\n        # Check if the aspect is mentioned in the review\n        if aspect in review:\n            text = f\"{review} What do you think about the {aspect}?\"\n            result = aspect_sentiment_pipeline(text)\n            aspect_sentiments[aspect] = {\n                'label': result[0]['label'],  # Positive, Neutral, or Negative\n                'score': result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']\n            }\n        else:\n            # Assign default value if aspect is not mentioned\n            aspect_sentiments[aspect] = {\n                'label': 'Not Mentioned',\n                'score': None\n            }\n    return aspect_sentiments\n# Apply the function using multithreading for efficiency\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(tqdm(executor.map(lambda x: analyze_aspect_sentiments_with_missing_handling(x, aspects), \n                                     df_sample['cleaned_review']), \n                        total=len(df_sample)))\n\n# Add results to the DataFrame\ndf_sample['aspect_sentiments'] = results\n\n# Extract aspect scores into a separate DataFrame\naspect_scores_df = pd.json_normalize(df_sample['aspect_sentiments'])\n\nprint(df_sample[['Review', 'aspect_sentiments']].head())\n\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\n/Users/yixin/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning:\n\n`torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  0%|          | 1/1000 [00:00&lt;01:49,  9.12it/s]  0%|          | 3/1000 [00:00&lt;02:34,  6.44it/s]  0%|          | 4/1000 [00:00&lt;03:36,  4.59it/s]  1%|          | 10/1000 [00:01&lt;01:21, 12.09it/s]  2%|▏         | 17/1000 [00:01&lt;01:06, 14.77it/s]  2%|▏         | 21/1000 [00:01&lt;01:14, 13.06it/s]  3%|▎         | 31/1000 [00:01&lt;00:45, 21.08it/s]  4%|▎         | 36/1000 [00:02&lt;00:42, 22.65it/s]  4%|▍         | 39/1000 [00:03&lt;01:33, 10.26it/s]  6%|▌         | 56/1000 [00:03&lt;00:50, 18.87it/s]  6%|▌         | 60/1000 [00:03&lt;00:45, 20.53it/s]  6%|▋         | 64/1000 [00:03&lt;00:46, 19.99it/s]  7%|▋         | 72/1000 [00:04&lt;00:45, 20.54it/s]  8%|▊         | 75/1000 [00:04&lt;00:58, 15.85it/s]  9%|▊         | 86/1000 [00:04&lt;00:35, 25.41it/s]  9%|▉         | 91/1000 [00:05&lt;00:40, 22.29it/s] 10%|█         | 100/1000 [00:05&lt;00:38, 23.16it/s] 10%|█         | 104/1000 [00:05&lt;00:39, 22.62it/s] 11%|█         | 109/1000 [00:06&lt;01:16, 11.69it/s] 12%|█▎        | 125/1000 [00:06&lt;00:37, 23.20it/s] 13%|█▎        | 132/1000 [00:07&lt;00:41, 20.77it/s] 14%|█▎        | 137/1000 [00:07&lt;00:40, 21.33it/s] 14%|█▍        | 142/1000 [00:07&lt;00:39, 21.87it/s] 15%|█▍        | 147/1000 [00:07&lt;00:41, 20.34it/s] 15%|█▌        | 150/1000 [00:08&lt;00:56, 15.12it/s] 16%|█▌        | 159/1000 [00:08&lt;00:36, 23.33it/s] 16%|█▋        | 164/1000 [00:08&lt;00:35, 23.68it/s] 17%|█▋        | 168/1000 [00:08&lt;00:36, 22.78it/s] 17%|█▋        | 172/1000 [00:09&lt;01:09, 11.94it/s] 19%|█▊        | 186/1000 [00:09&lt;00:38, 21.22it/s] 19%|█▉        | 191/1000 [00:10&lt;00:39, 20.69it/s] 19%|█▉        | 194/1000 [00:10&lt;00:43, 18.60it/s] 20%|█▉        | 197/1000 [00:10&lt;00:42, 19.08it/s] 20%|██        | 200/1000 [00:10&lt;00:51, 15.64it/s] 20%|██        | 202/1000 [00:11&lt;01:37,  8.21it/s] 22%|██▏       | 218/1000 [00:11&lt;00:37, 21.03it/s] 22%|██▏       | 223/1000 [00:12&lt;00:40, 19.18it/s] 23%|██▎       | 227/1000 [00:12&lt;00:37, 20.43it/s] 23%|██▎       | 231/1000 [00:12&lt;00:38, 19.74it/s] 24%|██▎       | 235/1000 [00:12&lt;00:40, 18.95it/s] 24%|██▍       | 238/1000 [00:13&lt;00:51, 14.85it/s] 25%|██▍       | 248/1000 [00:13&lt;00:37, 20.17it/s] 25%|██▌       | 254/1000 [00:13&lt;00:33, 22.44it/s] 26%|██▌       | 258/1000 [00:13&lt;00:31, 23.66it/s] 26%|██▌       | 261/1000 [00:14&lt;00:48, 15.32it/s] 27%|██▋       | 269/1000 [00:14&lt;00:32, 22.27it/s] 27%|██▋       | 273/1000 [00:14&lt;00:41, 17.37it/s] 28%|██▊       | 277/1000 [00:15&lt;00:37, 19.47it/s] 28%|██▊       | 281/1000 [00:15&lt;00:33, 21.31it/s] 28%|██▊       | 284/1000 [00:15&lt;00:35, 20.14it/s] 29%|██▉       | 288/1000 [00:15&lt;00:32, 22.01it/s] 29%|██▉       | 292/1000 [00:15&lt;00:29, 23.93it/s] 30%|██▉       | 295/1000 [00:16&lt;00:56, 12.45it/s] 30%|██▉       | 298/1000 [00:16&lt;00:48, 14.48it/s] 31%|███       | 306/1000 [00:16&lt;00:34, 20.03it/s] 31%|███       | 312/1000 [00:17&lt;01:01, 11.17it/s] 32%|███▏      | 324/1000 [00:17&lt;00:35, 18.98it/s] 33%|███▎      | 331/1000 [00:17&lt;00:28, 23.80it/s] 34%|███▎      | 336/1000 [00:18&lt;00:32, 20.58it/s] 34%|███▍      | 341/1000 [00:18&lt;00:29, 22.23it/s] 34%|███▍      | 345/1000 [00:18&lt;00:41, 15.85it/s] 35%|███▌      | 352/1000 [00:19&lt;00:33, 19.17it/s] 36%|███▌      | 358/1000 [00:19&lt;00:36, 17.39it/s] 36%|███▋      | 365/1000 [00:19&lt;00:27, 22.98it/s] 37%|███▋      | 370/1000 [00:19&lt;00:25, 25.09it/s] 37%|███▋      | 374/1000 [00:20&lt;00:30, 20.29it/s] 38%|███▊      | 379/1000 [00:20&lt;00:26, 23.51it/s] 38%|███▊      | 383/1000 [00:20&lt;00:35, 17.57it/s] 39%|███▉      | 388/1000 [00:21&lt;00:49, 12.33it/s] 39%|███▉      | 391/1000 [00:21&lt;00:48, 12.59it/s] 40%|████      | 400/1000 [00:21&lt;00:32, 18.69it/s] 40%|████      | 403/1000 [00:21&lt;00:29, 19.96it/s] 41%|████      | 407/1000 [00:22&lt;00:27, 21.29it/s] 41%|████      | 410/1000 [00:22&lt;00:28, 20.50it/s] 41%|████▏     | 413/1000 [00:22&lt;00:40, 14.34it/s] 42%|████▏     | 420/1000 [00:22&lt;00:31, 18.43it/s] 42%|████▎     | 425/1000 [00:23&lt;00:34, 16.87it/s] 43%|████▎     | 432/1000 [00:23&lt;00:32, 17.46it/s] 43%|████▎     | 434/1000 [00:23&lt;00:43, 13.09it/s] 44%|████▍     | 441/1000 [00:24&lt;00:35, 15.88it/s] 45%|████▍     | 446/1000 [00:24&lt;00:30, 17.95it/s] 45%|████▍     | 449/1000 [00:24&lt;00:41, 13.43it/s] 45%|████▌     | 453/1000 [00:25&lt;00:41, 13.03it/s] 46%|████▋     | 463/1000 [00:25&lt;00:33, 16.13it/s] 47%|████▋     | 466/1000 [00:26&lt;00:39, 13.55it/s] 47%|████▋     | 474/1000 [00:26&lt;00:36, 14.34it/s] 48%|████▊     | 481/1000 [00:26&lt;00:29, 17.49it/s] 48%|████▊     | 485/1000 [00:27&lt;00:29, 17.74it/s] 49%|████▉     | 489/1000 [00:27&lt;00:36, 13.87it/s] 50%|████▉     | 498/1000 [00:28&lt;00:34, 14.63it/s] 50%|█████     | 502/1000 [00:28&lt;00:30, 16.58it/s] 51%|█████     | 508/1000 [00:28&lt;00:24, 20.12it/s] 51%|█████     | 511/1000 [00:28&lt;00:23, 21.09it/s] 51%|█████▏    | 514/1000 [00:28&lt;00:22, 21.39it/s] 52%|█████▏    | 517/1000 [00:28&lt;00:23, 20.60it/s] 52%|█████▏    | 520/1000 [00:29&lt;00:30, 15.82it/s] 52%|█████▏    | 522/1000 [00:29&lt;00:30, 15.52it/s] 53%|█████▎    | 528/1000 [00:29&lt;00:21, 21.99it/s] 53%|█████▎    | 531/1000 [00:29&lt;00:25, 18.30it/s] 53%|█████▎    | 534/1000 [00:30&lt;00:35, 13.13it/s] 54%|█████▎    | 536/1000 [00:30&lt;00:38, 12.17it/s] 54%|█████▍    | 544/1000 [00:30&lt;00:22, 20.63it/s] 55%|█████▍    | 547/1000 [00:30&lt;00:21, 21.33it/s] 55%|█████▌    | 550/1000 [00:30&lt;00:19, 22.74it/s] 55%|█████▌    | 553/1000 [00:30&lt;00:21, 20.34it/s] 56%|█████▌    | 556/1000 [00:31&lt;00:24, 18.29it/s] 56%|█████▌    | 562/1000 [00:31&lt;00:17, 25.09it/s] 56%|█████▋    | 565/1000 [00:31&lt;00:32, 13.28it/s] 58%|█████▊    | 576/1000 [00:31&lt;00:16, 25.17it/s] 58%|█████▊    | 581/1000 [00:32&lt;00:17, 23.38it/s] 59%|█████▊    | 587/1000 [00:32&lt;00:17, 23.10it/s] 59%|█████▉    | 591/1000 [00:33&lt;00:33, 12.29it/s] 61%|██████    | 606/1000 [00:33&lt;00:17, 22.32it/s] 61%|██████    | 611/1000 [00:33&lt;00:16, 23.04it/s] 62%|██████▏   | 615/1000 [00:33&lt;00:18, 20.76it/s] 62%|██████▏   | 620/1000 [00:34&lt;00:24, 15.52it/s] 62%|██████▎   | 625/1000 [00:34&lt;00:20, 18.47it/s] 63%|██████▎   | 633/1000 [00:34&lt;00:18, 20.10it/s] 64%|██████▎   | 636/1000 [00:35&lt;00:19, 19.12it/s] 64%|██████▍   | 641/1000 [00:35&lt;00:20, 17.50it/s] 65%|██████▍   | 647/1000 [00:35&lt;00:15, 22.43it/s] 65%|██████▌   | 651/1000 [00:35&lt;00:18, 18.72it/s] 65%|██████▌   | 654/1000 [00:36&lt;00:30, 11.20it/s] 66%|██████▋   | 664/1000 [00:36&lt;00:19, 17.62it/s] 67%|██████▋   | 667/1000 [00:37&lt;00:18, 17.75it/s] 67%|██████▋   | 670/1000 [00:37&lt;00:19, 16.89it/s] 67%|██████▋   | 674/1000 [00:37&lt;00:28, 11.56it/s] 68%|██████▊   | 681/1000 [00:38&lt;00:19, 16.43it/s] 68%|██████▊   | 685/1000 [00:38&lt;00:22, 13.80it/s] 69%|██████▊   | 687/1000 [00:38&lt;00:24, 12.86it/s] 69%|██████▉   | 692/1000 [00:38&lt;00:18, 16.91it/s] 70%|██████▉   | 695/1000 [00:38&lt;00:17, 17.52it/s] 70%|██████▉   | 698/1000 [00:39&lt;00:20, 14.72it/s] 70%|███████   | 700/1000 [00:39&lt;00:26, 11.28it/s] 70%|███████   | 704/1000 [00:39&lt;00:23, 12.73it/s] 71%|███████   | 710/1000 [00:39&lt;00:15, 18.39it/s] 71%|███████▏  | 713/1000 [00:40&lt;00:15, 18.78it/s] 72%|███████▏  | 716/1000 [00:40&lt;00:13, 20.60it/s] 72%|███████▏  | 720/1000 [00:40&lt;00:22, 12.32it/s] 73%|███████▎  | 729/1000 [00:40&lt;00:13, 20.53it/s] 73%|███████▎  | 734/1000 [00:41&lt;00:11, 22.72it/s] 74%|███████▍  | 738/1000 [00:41&lt;00:10, 25.01it/s] 74%|███████▍  | 742/1000 [00:41&lt;00:14, 17.86it/s] 74%|███████▍  | 745/1000 [00:41&lt;00:14, 18.11it/s] 75%|███████▌  | 752/1000 [00:41&lt;00:10, 24.34it/s] 76%|███████▌  | 756/1000 [00:42&lt;00:11, 21.64it/s] 76%|███████▌  | 760/1000 [00:42&lt;00:10, 23.10it/s] 76%|███████▋  | 764/1000 [00:42&lt;00:09, 24.67it/s] 77%|███████▋  | 767/1000 [00:42&lt;00:11, 21.08it/s] 77%|███████▋  | 770/1000 [00:43&lt;00:21, 10.75it/s] 78%|███████▊  | 782/1000 [00:43&lt;00:10, 20.68it/s] 79%|███████▊  | 786/1000 [00:44&lt;00:13, 16.12it/s] 79%|███████▉  | 793/1000 [00:44&lt;00:10, 19.76it/s] 80%|███████▉  | 797/1000 [00:44&lt;00:09, 21.52it/s] 80%|████████  | 800/1000 [00:44&lt;00:08, 22.33it/s] 80%|████████  | 805/1000 [00:45&lt;00:16, 11.98it/s] 82%|████████▏ | 815/1000 [00:45&lt;00:10, 18.30it/s] 82%|████████▏ | 818/1000 [00:45&lt;00:10, 18.00it/s] 82%|████████▏ | 821/1000 [00:45&lt;00:09, 19.31it/s] 82%|████████▏ | 824/1000 [00:46&lt;00:10, 17.51it/s] 83%|████████▎ | 827/1000 [00:46&lt;00:11, 15.01it/s] 83%|████████▎ | 829/1000 [00:46&lt;00:14, 12.16it/s] 84%|████████▎ | 837/1000 [00:46&lt;00:08, 19.72it/s] 84%|████████▍ | 840/1000 [00:47&lt;00:12, 13.15it/s] 84%|████████▍ | 842/1000 [00:47&lt;00:11, 13.32it/s] 85%|████████▍ | 847/1000 [00:47&lt;00:09, 15.65it/s] 85%|████████▌ | 852/1000 [00:47&lt;00:07, 20.26it/s] 86%|████████▌ | 855/1000 [00:47&lt;00:06, 20.88it/s] 86%|████████▌ | 858/1000 [00:48&lt;00:11, 12.62it/s] 87%|████████▋ | 866/1000 [00:49&lt;00:10, 13.18it/s] 87%|████████▋ | 871/1000 [00:49&lt;00:09, 13.47it/s] 88%|████████▊ | 878/1000 [00:49&lt;00:08, 15.15it/s] 88%|████████▊ | 885/1000 [00:49&lt;00:05, 19.63it/s] 89%|████████▉ | 890/1000 [00:50&lt;00:06, 18.26it/s] 89%|████████▉ | 893/1000 [00:50&lt;00:05, 18.68it/s] 90%|████████▉ | 896/1000 [00:50&lt;00:05, 19.59it/s] 90%|█████████ | 900/1000 [00:50&lt;00:06, 14.73it/s] 91%|█████████ | 908/1000 [00:51&lt;00:04, 22.48it/s] 91%|█████████ | 912/1000 [00:51&lt;00:03, 22.15it/s] 92%|█████████▏| 917/1000 [00:51&lt;00:03, 23.04it/s] 92%|█████████▏| 921/1000 [00:51&lt;00:03, 25.29it/s] 92%|█████████▎| 925/1000 [00:52&lt;00:06, 12.04it/s] 94%|█████████▎| 935/1000 [00:52&lt;00:03, 20.34it/s] 94%|█████████▍| 939/1000 [00:52&lt;00:03, 16.50it/s] 94%|█████████▍| 942/1000 [00:53&lt;00:03, 15.59it/s] 95%|█████████▍| 946/1000 [00:53&lt;00:02, 18.01it/s] 95%|█████████▍| 949/1000 [00:53&lt;00:02, 19.62it/s] 95%|█████████▌| 953/1000 [00:54&lt;00:03, 11.87it/s] 96%|█████████▋| 964/1000 [00:54&lt;00:01, 22.76it/s] 97%|█████████▋| 970/1000 [00:54&lt;00:01, 27.73it/s] 98%|█████████▊| 975/1000 [00:54&lt;00:01, 21.73it/s] 98%|█████████▊| 981/1000 [00:54&lt;00:00, 26.94it/s] 99%|█████████▊| 986/1000 [00:55&lt;00:00, 15.31it/s] 99%|█████████▉| 991/1000 [00:56&lt;00:00, 12.92it/s]100%|██████████| 1000/1000 [00:56&lt;00:00, 17.82it/s]\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n                                                  Review  \\\n10957  worst exp, just got dissapointed, hotel midle ...   \n6644   great, ok place gets reviews run gamut right, ...   \n12619  did n't maid n't want re-write positive review...   \n15367  resort personality simpicity charm just got ba...   \n4290   wonderful stay, stayed hotel new year holiday ...   \n\n                                       aspect_sentiments  \n10957  {'location': {'label': 'Not Mentioned', 'score...  \n6644   {'location': {'label': 'Not Mentioned', 'score...  \n12619  {'location': {'label': 'NEGATIVE', 'score': -0...  \n15367  {'location': {'label': 'Not Mentioned', 'score...  \n4290   {'location': {'label': 'POSITIVE', 'score': 0....  \n\n\n\n\nCode\n# Count occurrences of each label for each aspect\nlabel_columns = [col for col in aspect_scores_df.columns if '.label' in col]\nlabel_counts = {}\n\nfor col in label_columns:\n    aspect = col.split('.')[0]\n    label_counts[aspect] = aspect_scores_df[col].value_counts()\n\n# Convert to DataFrame\nlabel_counts_df = pd.DataFrame(label_counts).fillna(0)\n\n# Plot a bar chart\nlabel_counts_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')\nplt.title(\"Sentiment Distribution Across Aspects\")\nplt.xlabel(\"Sentiment Labels\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.legend(title=\"Aspects\", loc='upper right')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Calculate mean scores for each aspect\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\n\n# Prepare heatmap data\nheatmap_data = pd.DataFrame(mean_scores).T\nheatmap_data.columns = [col.split('.')[0] for col in heatmap_data.columns]\n\n# Plot heatmap\nplt.figure(figsize=(10, 4))\nsns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\nplt.title(\"Mean Sentiment Scores for Aspects\")\nplt.xlabel(\"Aspects\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Calculate mean sentiment scores\nmean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()\naspects = [col.split('.')[0] for col in mean_scores.index]\n\n# Radar chart\nfig = go.Figure()\nfig.add_trace(go.Scatterpolar(\n    r=mean_scores,\n    theta=aspects,\n    fill='toself',\n    name='Average Sentiment Scores'\n))\n\nfig.update_layout(\n    polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),\n    title=\"Aspect-Based Sentiment Radar Chart\",\n    showlegend=True\n)\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.express as px\n\n# Prepare data for the sunburst chart\nlabel_data = []\nfor aspect in ['location', 'service', 'room', 'staff']:\n    labels = aspect_scores_df[f'{aspect}.label'].value_counts()\n    for label, count in labels.items():\n        label_data.append({'Aspect': aspect, 'Label': label, 'Count': count})\n\nlabel_df = pd.DataFrame(label_data)\n\n# Sunburst chart\nfig = px.sunburst(label_df, path=['Aspect', 'Label'], values='Count',\n                  title=\"Aspect-Based Sentiment Distribution (Sunburst)\",\n                  color='Count', color_continuous_scale='Viridis')\nfig.show()"
  }
]