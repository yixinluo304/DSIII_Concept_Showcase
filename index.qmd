---
title: "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews"
format:
  html:
    code-fold: true
jupyter: python3
---

Yixin Luo

## EDA

```{python}
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
from collections import Counter
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

## Load data
df = pd.read_csv('~/Downloads/tripadvisor_hotel_reviews.csv')


## distribution of ratings
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Rating', palette='viridis')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()
```

```{python}
## Check the length of reviews
df["word_length"] = df["Review"].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="word_length", hue="Rating", multiple="stack", palette="bright")
plt.title('Distribution of Text Lengths by Class', fontsize=16)
plt.xlabel('Text Length', fontsize=14)
plt.ylabel('Frequency')
plt.show()
```

```{python}
from collections import Counter
import pandas as pd
import re

# Assuming your DataFrame is named 'df' and the column with text is 'Review'

# Step 1: Preprocess the text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

df['cleaned_review'] = df['Review'].apply(clean_text)

# Step 2: Tokenize the text and count words
all_words = ' '.join(df['cleaned_review']).split()  # Combine all reviews and split into words
word_counts = Counter(all_words)  # Count word frequencies

# Step 3: Get the most common words
most_common_words = word_counts.most_common(20)  # Top 20 most common words
print("Most Common Words:", most_common_words)

# Convert to a DataFrame for better readability
most_common_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])
print(most_common_df)

# Horizontal visualization
plt.figure(figsize=(10, 6))
plt.barh(most_common_df['Word'], most_common_df['Frequency'], color='skyblue')
plt.title('Top 20 Most Common Words in Reviews', fontsize=16)
plt.xlabel('Frequency', fontsize=12)
plt.ylabel('Words', fontsize=12)
plt.gca().invert_yaxis()  # Invert y-axis to show highest frequency on top
plt.tight_layout()
plt.show()
```

## Text Cleaning

### remove the big comments

```{python}
# remove the big comments
df = df[df['word_length'] <= 3000]

df["word_length"] = df["Review"].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="word_length", hue="Rating", multiple="stack", palette="bright")
plt.title('Distribution of Text Lengths by Class', fontsize=16)
plt.xlabel('Text Length', fontsize=14)
plt.ylabel('Frequency')
plt.show()
```

```{python}
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk


# Text cleaning function
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply text cleaning
df.loc[:, 'cleaned_review'] = df['Review'].apply(clean_text)
print(df)
```

## Sentiment Analysis
```{python}
## Define aspects
aspects = ['location', 'service', 'room', 'staff']

import pandas as pd
from transformers import pipeline
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

df_sample = df.sample(1000, random_state=42) 
df_sample['cleaned_review'] = df_sample['Review'].apply(clean_text)

# Initialize DistilBERT pipeline
aspect_sentiment_pipeline = pipeline(
    "text-classification",
    model="distilbert-base-uncased-finetuned-sst-2-english",
    truncation=True,
    padding=True
)

# Function to handle missing aspects and perform sentiment analysis
def analyze_aspect_sentiments_with_missing_handling(review, aspects):
    aspect_sentiments = {}
    for aspect in aspects:
        # Check if the aspect is mentioned in the review
        if aspect in review:
            text = f"{review} What do you think about the {aspect}?"
            result = aspect_sentiment_pipeline(text)
            aspect_sentiments[aspect] = {
                'label': result[0]['label'],  # Positive, Neutral, or Negative
                'score': result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']
            }
        else:
            # Assign default value if aspect is not mentioned
            aspect_sentiments[aspect] = {
                'label': 'Not Mentioned',
                'score': None
            }
    return aspect_sentiments
# Apply the function using multithreading for efficiency
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(tqdm(executor.map(lambda x: analyze_aspect_sentiments_with_missing_handling(x, aspects), 
                                     df_sample['cleaned_review']), 
                        total=len(df_sample)))

# Add results to the DataFrame
df_sample['aspect_sentiments'] = results

# Extract aspect scores into a separate DataFrame
aspect_scores_df = pd.json_normalize(df_sample['aspect_sentiments'])

print(df_sample[['Review', 'aspect_sentiments']].head())
```

```{python}
# Count occurrences of each label for each aspect
label_columns = [col for col in aspect_scores_df.columns if '.label' in col]
label_counts = {}

for col in label_columns:
    aspect = col.split('.')[0]
    label_counts[aspect] = aspect_scores_df[col].value_counts()

# Convert to DataFrame
label_counts_df = pd.DataFrame(label_counts).fillna(0)

# Plot a bar chart
label_counts_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')
plt.title("Sentiment Distribution Across Aspects")
plt.xlabel("Sentiment Labels")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.legend(title="Aspects", loc='upper right')
plt.show()
```

```{python}
# Calculate mean scores for each aspect
mean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()

# Prepare heatmap data
heatmap_data = pd.DataFrame(mean_scores).T
heatmap_data.columns = [col.split('.')[0] for col in heatmap_data.columns]

# Plot heatmap
plt.figure(figsize=(10, 4))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", cbar=True, fmt=".2f")
plt.title("Mean Sentiment Scores for Aspects")
plt.xlabel("Aspects")
plt.ylabel("")
plt.show()
```

```{python}
import plotly.graph_objects as go
import numpy as np

# Calculate mean sentiment scores
mean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()
aspects = [col.split('.')[0] for col in mean_scores.index]

# Radar chart
fig = go.Figure()
fig.add_trace(go.Scatterpolar(
    r=mean_scores,
    theta=aspects,
    fill='toself',
    name='Average Sentiment Scores'
))

fig.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),
    title="Aspect-Based Sentiment Radar Chart",
    showlegend=True
)
fig.show()
```

```{python}
import plotly.express as px

# Prepare data for the sunburst chart
label_data = []
for aspect in ['location', 'service', 'room', 'staff']:
    labels = aspect_scores_df[f'{aspect}.label'].value_counts()
    for label, count in labels.items():
        label_data.append({'Aspect': aspect, 'Label': label, 'Count': count})

label_df = pd.DataFrame(label_data)

# Sunburst chart
fig = px.sunburst(label_df, path=['Aspect', 'Label'], values='Count',
                  title="Aspect-Based Sentiment Distribution (Sunburst)",
                  color='Count', color_continuous_scale='Viridis')
fig.show()
```
