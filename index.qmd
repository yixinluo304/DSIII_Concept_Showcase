---
title: "Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews"
format:
  html:
    code-fold: true
jupyter: python3
---

Yixin Luo

## **Understanding Guest Experiences: Sentiment and N-gram Analysis of TripAdvisor Hotel Reviews**

This project analyzes customer feedback through **aspect-based sentiment analysis** and **n-gram analysis**, providing actionable insights into guest experiences. Key aspects like **location**, **service**, **room**, and **staff** were evaluated to identify strengths, weaknesses, and trends in reviews.

### **Key Methods**

-   **Aspect-Based Sentiment Analysis**: Using **Hugging Face's DistilBERT**, I classified sentiments for each aspect, supported by **multithreading** for scalability.
-   **Term Frequency and N-gram Analysis**: Common words, bigrams, and trigrams were identified, normalized, and visualized to uncover recurring themes and guest priorities.

### **Why This Matters**

By combining sentiment and linguistic analyses, this approach enables businesses to: - Understand guest preferences and concerns. - Improve service quality with data-driven insights. - Uncover patterns in customer feedback to enhance guest satisfaction.

------------------------------------------------------------------------

## EDA

```{python}
import torch
```

```{python}
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
from collections import Counter
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

## Load data
df = pd.read_csv('~/Downloads/tripadvisor_hotel_reviews.csv')


## distribution of ratings
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Rating', palette='viridis')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()
```

#### Distribution of Ratings: The bar chart displays the frequency of ratings from 1 to 5, highlighting that the majority of users gave a rating of 5.

```{python}
## Check the length of reviews
df["word_length"] = df["Review"].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="word_length", hue="Rating", multiple="stack", palette="bright")
plt.title('Distribution of Text Lengths by Class', fontsize=16)
plt.xlabel('Text Length', fontsize=14)
plt.ylabel('Frequency')
plt.show()
```

#### Distribution of Text Lengths by Class: The histogram shows the distribution of review text lengths, segmented by rating class, revealing that most reviews are short across all ratings.

```{python}
from collections import Counter
import pandas as pd
import re

# Step 1: Preprocess the text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

df['cleaned_review'] = df['Review'].apply(clean_text)

# Step 2: Tokenize the text and count words
all_words = ' '.join(df['cleaned_review']).split()  # Combine all reviews and split into words
word_counts = Counter(all_words)  # Count word frequencies

# Step 3: Get the most common words
most_common_words = word_counts.most_common(20)  # Top 20 most common words
print("Most Common Words:", most_common_words)

# Convert to a DataFrame for better readability
most_common_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])
print(most_common_df)

# Horizontal visualization
plt.figure(figsize=(10, 6))
plt.barh(most_common_df['Word'], most_common_df['Frequency'], color='skyblue')
plt.title('Top 20 Most Common Words in Reviews', fontsize=16)
plt.xlabel('Frequency', fontsize=12)
plt.ylabel('Words', fontsize=12)
plt.gca().invert_yaxis()  # Invert y-axis to show highest frequency on top
plt.tight_layout()
plt.show()
```

#### Top 20 Most Common Words in Reviews: The bar chart identifies the most frequently used words in hotel reviews, with "hotel" and "room" being the most common, indicating key topics of discussion.

## Text Cleaning

### remove the big comments

```{python}
# remove the big comments
df = df[df['word_length'] <= 3000]

df["word_length"] = df["Review"].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="word_length", hue="Rating", multiple="stack", palette="bright")
plt.title('Distribution of Text Lengths by Class', fontsize=16)
plt.xlabel('Text Length', fontsize=14)
plt.ylabel('Frequency')
plt.show()
```

```{python}
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk


# Text cleaning function
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply text cleaning
df.loc[:, 'cleaned_review'] = df['Review'].apply(clean_text)
print(df)
```

## **Aspect-Based Sentiment Analysis**

I implemented an **aspect-based sentiment analysis** to uncover insights from hotel reviews, focusing on key aspects: **location**, **service**, **room**, and **staff**. This analysis helps break down overall feedback into actionable insights for each category.

### **Key Steps**

1.  **Data Preprocessing**: Cleaned and prepared reviews for analysis to ensure consistency.
2.  **Sentiment Classification**: Used **Hugging Face's DistilBERT**, a cutting-edge machine learning model, to classify sentiments as positive or negative for each aspect.
3.  **Aspect-Specific Analysis**:
    -   For each review, I generated prompts like, *"What do you think about the service?"* to extract aspect-focused sentiment.
    -   If an aspect wasn't mentioned, it was marked as **'Not Mentioned'**.

### **Efficiency at Scale**

To analyze thousands of reviews efficiently, I employed **multithreading**, speeding up the process while maintaining accuracy.

### **Data Visualization**

I structured the results into interactive visualizations, including: - **Sentiment Scores**: Displaying the average sentiment for each aspect. - **Radar Charts**: Providing an intuitive overview of customer perceptions.

```{python}
## Define aspects
aspects = ['location', 'service', 'room', 'staff']

import pandas as pd
from transformers import pipeline
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

df_sample = df.sample(1000, random_state=42) 
df_sample['cleaned_review'] = df_sample['Review'].apply(clean_text)

aspect_sentiment_pipeline = pipeline(
    "text-classification",
    model="distilbert-base-uncased-finetuned-sst-2-english",
    framework="pt",  # Force PyTorch
    truncation=True,
    padding=True
)

# Function to handle missing aspects and perform sentiment analysis
def analyze_aspect_sentiments_with_missing_handling(review, aspects):
    aspect_sentiments = {}
    for aspect in aspects:
        # Check if the aspect is mentioned in the review
        if aspect in review:
            text = f"{review} What do you think about the {aspect}?"
            result = aspect_sentiment_pipeline(text)
            aspect_sentiments[aspect] = {
                'label': result[0]['label'],  # Positive, Neutral, or Negative
                'score': result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']
            }
        else:
            # Assign default value if aspect is not mentioned
            aspect_sentiments[aspect] = {
                'label': 'Not Mentioned',
                'score': None
            }
    return aspect_sentiments
# Apply the function using multithreading for efficiency
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(tqdm(executor.map(lambda x: analyze_aspect_sentiments_with_missing_handling(x, aspects), 
                                     df_sample['cleaned_review']), 
                        total=len(df_sample)))

# Add results to the DataFrame
df_sample['aspect_sentiments'] = results

# Extract aspect scores into a separate DataFrame
aspect_scores_df = pd.json_normalize(df_sample['aspect_sentiments'])

print(df_sample[['Review', 'aspect_sentiments']].head())
```

```{python}
# Count occurrences of each label for each aspect
label_columns = [col for col in aspect_scores_df.columns if '.label' in col]
label_counts = {}

for col in label_columns:
    aspect = col.split('.')[0]
    label_counts[aspect] = aspect_scores_df[col].value_counts()

# Convert to DataFrame
label_counts_df = pd.DataFrame(label_counts).fillna(0)

# Plot a bar chart
label_counts_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')
plt.title("Sentiment Distribution Across Aspects")
plt.xlabel("Sentiment Labels")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.legend(title="Aspects", loc='upper right')
plt.show()
```

```{python}
# Calculate mean scores for each aspect
mean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()

# Prepare heatmap data
heatmap_data = pd.DataFrame(mean_scores).T
heatmap_data.columns = [col.split('.')[0] for col in heatmap_data.columns]

# Plot heatmap
plt.figure(figsize=(10, 4))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", cbar=True, fmt=".2f")
plt.title("Mean Sentiment Scores for Aspects")
plt.xlabel("Aspects")
plt.ylabel("")
plt.show()
```

```{python}
import plotly.graph_objects as go
import numpy as np

# Calculate mean sentiment scores
mean_scores = aspect_scores_df[[col for col in aspect_scores_df.columns if '.score' in col]].mean()
aspects = [col.split('.')[0] for col in mean_scores.index]

# Radar chart
fig = go.Figure()
fig.add_trace(go.Scatterpolar(
    r=mean_scores,
    theta=aspects,
    fill='toself',
    name='Average Sentiment Scores'
))

fig.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),
    title="Aspect-Based Sentiment Radar Chart",
    showlegend=True
)
fig.show()
```

```{python}
import plotly.express as px

# Prepare data for the sunburst chart
label_data = []
for aspect in ['location', 'service', 'room', 'staff']:
    labels = aspect_scores_df[f'{aspect}.label'].value_counts()
    for label, count in labels.items():
        label_data.append({'Aspect': aspect, 'Label': label, 'Count': count})

label_df = pd.DataFrame(label_data)

# Sunburst chart
fig = px.sunburst(label_df, path=['Aspect', 'Label'], values='Count',
                  title="Aspect-Based Sentiment Distribution (Sunburst)",
                  color='Count', color_continuous_scale='Viridis')
fig.show()
```

## **Term Frequency and N-gram Analysis**

In addition to sentiment analysis, I explored the linguistic patterns in guest reviews through term frequency and n-gram analysis (bigrams and trigrams). These techniques highlight recurring phrases that reflect customer priorities and recurring themes.

### **Key Steps**

1.  **Term Frequency**:

-   Tokenized and cleaned text data to calculate the frequency of individual words.
-   Identified the most common words, excluding generic stopwords, to reveal dominant topics like "service," "clean," and "location."

2.  **N-gram Analysis**:

-   Extracted bigrams (two-word combinations) and trigrams (three-word combinations) to capture contextual patterns.
-   Normalized n-grams (e.g., treating "great hotel" and "hotel great" as identical) for more meaningful insights.
-   Applied manual filters to exclude irrelevant or uninformative n-grams (e.g., "ca nt," "did not").

```{python}
from nltk.util import ngrams
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from collections import Counter
import nltk

# Step 1: Download Stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Function to clean text (if not already done)
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

# Assuming 'df' contains the 'Review' column
# Preprocess reviews
df['cleaned_review'] = df['Review'].apply(clean_text)

# Function to generate n-grams from text
def generate_ngrams(text, n):
    tokens = text.split()  # Split text into words
    return list(ngrams(tokens, n))  # Generate n-grams

# Step 2: Generate bigrams and trigrams
df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))  # Bigrams
df['trigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 3))  # Trigrams

# Step 3: Flatten bigrams and trigrams into lists
all_bigrams = [bigram for bigram_list in df['bigrams'] for bigram in bigram_list]
all_trigrams = [trigram for trigram_list in df['trigrams'] for trigram in trigram_list]

# Step 4: Normalize bigrams and trigrams (sort alphabetically to handle duplicates)
def normalize_ngram(ngram):
    return tuple(sorted(ngram))  # Sort words alphabetically in each n-gram

normalized_bigrams = [normalize_ngram(bigram) for bigram in all_bigrams]
normalized_trigrams = [normalize_ngram(trigram) for trigram in all_trigrams]

# Step 5: Count frequencies of normalized bigrams and trigrams
bigram_counts = Counter(normalized_bigrams)
trigram_counts = Counter(normalized_trigrams)

# Step 6: Exclude unwanted bigrams
exclude_bigrams = [('ca', 'nt'), ('did', 'nt'), ('did', 'not')]

def filter_exclude_bigrams(bigram_counts, exclude_list):
    filtered_bigrams = {bigram: count for bigram, count in bigram_counts.items() if bigram not in exclude_list}
    return filtered_bigrams

filtered_bigram_counts = filter_exclude_bigrams(bigram_counts, exclude_bigrams)

# Step 7: Get the most common bigrams and trigrams
most_common_filtered_bigrams = Counter(filtered_bigram_counts).most_common(20)
most_common_trigrams = trigram_counts.most_common(20)

# Step 8: Convert results to DataFrames for visualization
filtered_bigram_df = pd.DataFrame(most_common_filtered_bigrams, columns=['Bigram', 'Frequency'])
filtered_bigram_df['Bigram'] = filtered_bigram_df['Bigram'].apply(lambda x: ' '.join(x))  # Convert tuples to strings

trigram_df = pd.DataFrame(most_common_trigrams, columns=['Trigram', 'Frequency'])
trigram_df['Trigram'] = trigram_df['Trigram'].apply(lambda x: ' '.join(x))  # Convert tuples to strings

# Step 9: Visualization for Bigrams
colors = plt.cm.viridis(np.linspace(0, 1, len(filtered_bigram_df)))  # Generate colors from a colormap

plt.figure(figsize=(10, 6))
plt.barh(filtered_bigram_df['Bigram'], filtered_bigram_df['Frequency'], color=colors)
plt.title('Top 20 Filtered Bigrams in Reviews', fontsize=16)
plt.xlabel('Frequency', fontsize=12)
plt.ylabel('Bigrams', fontsize=12)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Step 10: Visualization for Trigrams
colors = plt.cm.plasma(np.linspace(0, 1, len(trigram_df)))  # Use a different colormap for trigrams

plt.figure(figsize=(10, 6))
plt.barh(trigram_df['Trigram'], trigram_df['Frequency'], color=colors)
plt.title('Top 20 Trigrams in Reviews', fontsize=16)
plt.xlabel('Frequency', fontsize=12)
plt.ylabel('Trigrams', fontsize=12)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```
